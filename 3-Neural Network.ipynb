{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/PAE-ITBA-ML-2019/Regularizacion/master/reg_helper.py\n",
    "! wget https://raw.githubusercontent.com/PAE-ITBA-ML-2019/Regularizacion/master/fnn_helper.py\n",
    "! wget https://raw.githubusercontent.com/PAE-ITBA-ML-2019/Regularizacion/master/draw_nn.py\n",
    "! wget https://raw.githubusercontent.com/PAE-ITBA-ML-2019/Regularizacion/master/regularization_helper.py\n",
    "\n",
    "! wget https://github.com/PAE-ITBA-ML-2019/Regularizacion/raw/master/data.zip\n",
    "! unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste de la cantidad de parámetros de una Red Neuronal\n",
    "# L2 (Ridge regression)\n",
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.display import HTML\n",
    "import reg_helper as RHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import draw_nn\n",
    "from matplotlib import pyplot as plt \n",
    "folder = 'data/'\n",
    "X_train = np.load(folder+'X_train.npy')\n",
    "X_test = np.load(folder+'X_test.npy')\n",
    "y_train = np.load(folder+'y_train.npy') \n",
    "y_test = np.load(folder+'y_test.npy')\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,6))\n",
    "RHelper.plot_boundaries(X_train, y_train, ax=ax1)\n",
    "RHelper.plot_boundaries(X_test, y_test, ax=ax2)\n",
    "ax1.set_title('Train Data')\n",
    "ax2.set_title('CV Data')\n",
    "ax1.set_xlabel('$X_1$')\n",
    "ax1.set_ylabel('$X_2$')\n",
    "ax2.set_xlabel('$X_1$')\n",
    "ax2.set_ylabel('$X_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = 2\n",
    "hidden_units = 20 # Probar con 3, 10, 20, 200\n",
    "output_size = 1\n",
    "network = draw_nn.DrawNN( [input_shape, hidden_units, output_size] )\n",
    "network.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"./images/chips-mlp-20-hidden-1.mp4\" width=\"480\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de red neuronal con todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense\n",
    "from keras import regularizers\n",
    "from keras.constraints import max_norm\n",
    "initializer='normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_norm_value = 10\n",
    "dropout = 0.5\n",
    "l2_lambda = 1e-4\n",
    "activation = 'sigmoid'\n",
    "\n",
    "model = Sequential()\n",
    "regularizer = regularizers.l2(l2_lambda)\n",
    "model.add(Dense(hidden_units,input_dim=input_shape,  \n",
    "                kernel_initializer=initializer, \n",
    "                bias_initializer=initializer,\n",
    "                activation=activation, \n",
    "                kernel_regularizer=regularizer, \n",
    "                bias_regularizer=regularizer,\n",
    "                kernel_constraint=max_norm(max_norm_value),\n",
    "                bias_constraint=max_norm(max_norm_value)\n",
    "               ))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(output_size, \n",
    "            activation='sigmoid', \n",
    "            kernel_initializer=initializer, \n",
    "            bias_initializer=initializer,\n",
    "            name='Salida',\n",
    "            kernel_regularizer=regularizer, \n",
    "            bias_regularizer=regularizer\n",
    "           ))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La llamaremos desde la librería por comodidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_layer_model = RHelper.get_two_layer_model_L2(input_shape, output_size, hidden_units=hidden_units, lr=0.1, l2_lambda=0, decay=0.0)\n",
    "two_layer_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Que opciones tenemos con Keras en una red neuronal:\n",
    "\n",
    "- Regresión de Ridge L2 o L1\n",
    "- max_norm (Constraints - Restricciones)\n",
    "- Dropout\n",
    "- Early stoping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probar Relu y sigmoidea y ver diferencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fnn_helper import PlotLosses\n",
    "from keras import optimizers\n",
    "epochs = 2000 #adam 3000, sgd 30000\n",
    "lr = 0.1\n",
    "decay=0\n",
    "hidden_units = 100\n",
    "optim = optimizers.Adam(lr=lr) # cambiar a momentum 0.9 y lr a 5\n",
    "batch_size = 59 # 10 y 59\n",
    "activation='relu'\n",
    "l2_lambda = 1e-4\n",
    "dropout = None # 0.5\n",
    "max_norm_value = None # 5 # None\n",
    "\n",
    "plot_losses = PlotLosses(plot_interval=200, evaluate_interval=None, x_val=X_test, y_val_categorical=y_test)\n",
    "two_layer_model = RHelper.get_two_layer_model_L2(input_shape, \n",
    "                                                 output_size, \n",
    "                                                 hidden_units=hidden_units, \n",
    "                                                 # lr=lr, \n",
    "                                                 l2_lambda=l2_lambda, \n",
    "                                                 dropout=dropout,\n",
    "                                                 decay=decay,\n",
    "                                                 optim = optim,\n",
    "                                                 activation=activation, \n",
    "                                                 max_norm_value=max_norm_value\n",
    "                                                )\n",
    "two_layer_model.fit(X_train, \n",
    "          y_train, batch_size = batch_size,\n",
    "          epochs=epochs, \n",
    "          verbose=0, \n",
    "          validation_data=(X_test, y_test), \n",
    "          callbacks=[plot_losses],\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Que diferencias se ven entre?\n",
    "- ADAM y SGD. En cuanto a tiempos, curva de entrenamiento (monotona decreciente?)\n",
    "- mini-batch size?\n",
    "- con el decay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para calcular norma en cada epoch\n",
    "def get_weights_array(ws):\n",
    "    weights_norm = []\n",
    "    for weights in ws:\n",
    "        weights_norm.append(np.linalg.norm(weights[0]))\n",
    "    return weights_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two_layer_model.load_weights('SGD-3000-epochs-20-hu.hdf5')\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,6))\n",
    "RHelper.plot_boundaries_keras(X_train, y_train, two_layer_model.evaluate(X_train, y_train)[1], two_layer_model.predict_proba, h = 0.01, margin=0.1, ax=ax1)\n",
    "RHelper.plot_boundaries_keras(X_test, y_test, two_layer_model.evaluate(X_test, y_test)[1], two_layer_model.predict_proba, h = 0.01, margin=0.1, ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(RHelper.get_weights_array(plot_losses.weights))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Weights norm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diferencias fundamentales con Regresión logística\n",
    "- No hay garantía de converger al mínimo absoluto\n",
    "- Cada corrida puede dar resultados diferentes debido a la inicialización\n",
    "- La función de activación tiene gran influencia en el resultado del modelo obtenido\n",
    "- La regularización se aplica a nivel de capa\n",
    "- Existe el concepto de regularización de activaciones y keras lo soporta (https://github.com/keras-team/keras/issues/1618)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Elimina conexiones aleatoriamente al momento de entrenamiento (Especificamente, multiplica por cero algunas de las activaciones a la salida de una capa o eventualemente a la entrada de la red, aunque esto no es tan usual)\n",
    "- keep_prob es la probabilidad de que las conexiones no se eliminen\n",
    "- En keras se define el dropout_rate que es (1 - keep_prob)\n",
    "- Al momento de entrenamiento, cuando se eliminan las conexiones, se dividen las salidas de las activaciones por 1/keep_prob para mantener en promedio lo que llega a la capa siguiente \n",
    "- En momento de evaluación no se eliminan conexiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/dropout.png\" alt=\"Drawing\" style=\"width:60%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tips:**  \n",
    "- Usar un valor bajo entre 20% y 50%\n",
    "- Usar una \"red grande\", muchos parámetros\n",
    "- Usarlo tanto en la capa visible (entrada) como en las capas ocultas. La aplicación de dropout en cada capa a mostrado buenos resultados\n",
    "- Usar un learning rate grande con decay y momentum. Incrementar el learning rate por un factor de 10 o hasta 100 y usar momentum del entre 0.9 y 0.99.\n",
    "- Usar max-norm. Valores en el orden de 4 o 5 han mostrado buenos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencias: [[1]](https://www.coursera.org/learn/deep-neural-network/lecture/eM33A/dropout-regularization), [[2]](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Busqueda de cantidad de parámetros\n",
    "\n",
    "Correr varias veces y cambiar relu por sigmoidea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hidden_units in [2, 3, 5, 10, 20, 100]:\n",
    "    epochs = 2000 #adam 3000, sgd 30000\n",
    "    lr = 0.1\n",
    "    l2_lambda=0 #1e-3\n",
    "    dropout=0\n",
    "    decay=0\n",
    "    # hidden_units = 20\n",
    "    optim = optimizers.Adam(lr=lr) # cambiar a momentum 0.9 y lr a 5\n",
    "    batch_size = 59 # 10 y 59\n",
    "    activation='relu'\n",
    "\n",
    "    plot_losses = PlotLosses(plot_interval=200, evaluate_interval=None, x_val=X_test, y_val_categorical=y_test)\n",
    "    two_layer_model = RHelper.get_two_layer_model_L2(input_shape, \n",
    "                                                     output_size, \n",
    "                                                     hidden_units=hidden_units, \n",
    "                                                     # lr=lr, \n",
    "                                                     l2_lambda=l2_lambda, \n",
    "                                                     decay=decay,\n",
    "                                                     optim = optim,\n",
    "                                                     activation=activation,\n",
    "                                                     dropout=dropout\n",
    "                                                    )\n",
    "    two_layer_model.fit(X_train, \n",
    "              y_train, batch_size = batch_size,\n",
    "              epochs=epochs, \n",
    "              verbose=0, \n",
    "              validation_data=(X_test, y_test), \n",
    "              callbacks=[],\n",
    "             )\n",
    "    print(f'cantidad de unidades ocultas: {hidden_units}, función de activación: {activation}')\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,6))\n",
    "    RHelper.plot_boundaries_keras(X_train, y_train, two_layer_model.evaluate(X_train, y_train, verbose=0)[1], two_layer_model.predict_proba, h = 0.01, margin=0.1, ax=ax1)\n",
    "    RHelper.plot_boundaries_keras(X_test, y_test, two_layer_model.evaluate(X_test, y_test, verbose=0)[1], two_layer_model.predict_proba, h = 0.01, margin=0.1, ax=ax2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hidden_units in [2, 3, 5, 10, 20, 100]:\n",
    "    epochs = 2000 #adam 3000, sgd 30000\n",
    "    lr = 0.1\n",
    "    decay=0\n",
    "    l2_lambda=0\n",
    "    dropout=0\n",
    "    # hidden_units = 20\n",
    "    optim = optimizers.Adam(lr=lr) # cambiar a momentum 0.9 y lr a 5\n",
    "    batch_size = 59 # 10 y 59\n",
    "    activation='sigmoid'\n",
    "\n",
    "    plot_losses = PlotLosses(plot_interval=200, evaluate_interval=None, x_val=X_test, y_val_categorical=y_test)\n",
    "    two_layer_model = RHelper.get_two_layer_model_L2(input_shape, \n",
    "                                                     output_size, \n",
    "                                                     hidden_units=hidden_units, \n",
    "                                                     # lr=lr, \n",
    "                                                     l2_lambda=l2_lambda, \n",
    "                                                     decay=decay,\n",
    "                                                     optim = optim,\n",
    "                                                     activation=activation,\n",
    "                                                     dropout=dropout\n",
    "                                                    )\n",
    "    two_layer_model.fit(X_train, \n",
    "              y_train, batch_size = batch_size,\n",
    "              epochs=epochs, \n",
    "              verbose=0, \n",
    "              validation_data=(X_test, y_test), \n",
    "              callbacks=[],\n",
    "             )\n",
    "    print(f'cantidad de unidades ocultas: {hidden_units}, función de activación: {activation}')\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,6))\n",
    "    RHelper.plot_boundaries_keras(X_train, y_train, two_layer_model.evaluate(X_train, y_train, verbose=0)[1], two_layer_model.predict_proba, h = 0.01, margin=0.1, ax=ax1)\n",
    "    RHelper.plot_boundaries_keras(X_test, y_test, two_layer_model.evaluate(X_test, y_test, verbose=0)[1], two_layer_model.predict_proba, h = 0.01, margin=0.1, ax=ax2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
