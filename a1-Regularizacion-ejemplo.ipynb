{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/PAE-ITBA-ML-2019/Regularizacion/master/reg_helper.py\n",
    "! wget https://raw.githubusercontent.com/PAE-ITBA-ML-2019/Regularizacion/master/fnn_helper.py\n",
    "! wget https://raw.githubusercontent.com/PAE-ITBA-ML-2019/Regularizacion/master/draw_nn.py\n",
    "! wget https://raw.githubusercontent.com/PAE-ITBA-ML-2019/Regularizacion/master/regularization_helper.py\n",
    " \n",
    "! wget https://github.com/PAE-ITBA-ML-2019/Regularizacion/raw/master/data.zip\n",
    "! unzip data.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from regularization_helper import plot_it, get_simple_dataset, get_polynimial_set, plot_classifier, save_dataset\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras import optimizers\n",
    "from fnn_helper import PlotLosses\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargo dataset y lo grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_200 = np.load('200_samples_OK.npy')\n",
    "X = dataset_200[:,:2]\n",
    "y = dataset_200[:, 2]\n",
    "plot_it(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divido en training y testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y, test_size=0.4, shuffle=True, random_state=500)\n",
    "y_train = np.array(y_train)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de una sola capa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armo arquitectura de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_model(input_shape, output_size, lr=0.1):\n",
    "    model = Sequential()\n",
    "    sgd = optimizers.SGD(lr=lr)\n",
    "    model.add(Dense(output_size, input_dim=input_shape,\n",
    "                    activation='sigmoid', \n",
    "                    kernel_initializer='normal', \n",
    "                    name='Salida'\n",
    "                   ))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "basic_model = get_basic_model(X_train.shape[1], 1)\n",
    "basic_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entreno el modelo\n",
    "- Usar distintos valores de lr 0.1, 0.05, 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses = PlotLosses(plot_interval=50, evaluate_interval=None, x_val=X_test, y_val_categorical=y_test)\n",
    "basic_model = get_basic_model(X_train.shape[1], 1, lr=0.01)\n",
    "basic_model.fit(X_train, \n",
    "          y_train, batch_size = 25,\n",
    "          epochs=5000, \n",
    "          verbose=0, \n",
    "          validation_data=(X_test, y_test), \n",
    "          callbacks=[plot_losses],\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafico resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(model, X_train, y_train, X_test, y_test, N=200, figsize=(20,5)):\n",
    "    score_train = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train set')\n",
    "    print(\"loss: \", score_train[0])\n",
    "    print(\"accuracy: \", score_train[1])\n",
    "    score_test = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print()\n",
    "    print('Test set')\n",
    "    print(\"loss: \", score_test[0])\n",
    "    print(\"accuracy: \", score_test[1])\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, sharex='col', sharey='row', figsize=figsize)\n",
    "    plot_classifier(X_train, y_train, model.predict, 1, N = N, ax=ax1)\n",
    "    plot_classifier(X_test, y_test, model.predict, 1, N = N, ax=ax2)\n",
    "    plt.show()\n",
    "    return score_train, score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_basic_model = plot_results(basic_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "- ¿Es posible mejorar el modelo?\n",
    "- UNDERFITTING\n",
    "- Proponga dos posibles soluciones para mejorar el modelo: Agrandar arquitectura de la red, Regresión polinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de dos capas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armo arquitectura de la red (10 hidden units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_layer_model(input_shape, output_size, hidden_units= 10, lr=0.1, decay=0.0):\n",
    "    model = Sequential()\n",
    "    sgd = optimizers.SGD(lr=lr, decay=decay)\n",
    "    model.add(Dense(hidden_units,input_dim=input_shape,  activation='sigmoid', ))\n",
    "    model.add(Dense(output_size, \n",
    "                    activation='sigmoid', \n",
    "                    kernel_initializer='zeros', \n",
    "                    name='Salida'\n",
    "                   ))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "two_layer_model = get_two_layer_model(X_train.shape[1], 1, lr=1)\n",
    "two_layer_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entreno el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notar la diferencia entre lr entre este modelo y el anterior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses = PlotLosses(plot_interval=50, evaluate_interval=None, x_val=X_test, y_val_categorical=y_test)\n",
    "checkpointer = ModelCheckpoint(filepath='two_layer_model.hdf5', verbose=0, save_best_only=True)\n",
    "two_layer_model = get_two_layer_model(X_train.shape[1], 1, lr=2)\n",
    "two_layer_model.fit(X_train, \n",
    "          y_train, batch_size = 25,\n",
    "          epochs=3500, \n",
    "          verbose=0, \n",
    "          validation_data=(X_test, y_test), \n",
    "          callbacks=[plot_losses, checkpointer],\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafico resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_two_layer_model = plot_results(two_layer_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones:\n",
    "- OVERFITTING\n",
    "- ¿Posibles soluciones? Early Stop? Salir a buscar mas datos? Regularización? Dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soluciones para el overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_layer_model_early_stop = get_two_layer_model(X_train.shape[1], 1, lr=2)\n",
    "two_layer_model_early_stop.load_weights('two_layer_model.hdf5')\n",
    "score_two_layer_model_early_stop = plot_results(two_layer_model_early_stop, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_layer_model_early_stop.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Tenemos mas data?\n",
    "Cantidad de muestras mucho mayor a la cantidad de parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1000 = np.load('1000_samples.npy')\n",
    "X_1000 = dataset_1000[:,:2]\n",
    "y_1000 = dataset_1000[:, 2]\n",
    "plot_it(X_1000,y_1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1000_train, X_1000_test, y_1000_train, y_1000_test = model_selection.train_test_split(X_1000,y_1000, test_size=0.4, shuffle=True, random_state=500)\n",
    "print(X_1000_train.shape)\n",
    "print(X_1000_test.shape)\n",
    "print(y_1000_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses = PlotLosses(plot_interval=50, evaluate_interval=None, x_val=X_1000_test, y_val_categorical=y_1000_test)\n",
    "checkpointer = ModelCheckpoint(filepath='two_layer_model.1000.hdf5', verbose=0, save_best_only=True)\n",
    "two_layer_model_1000 = get_two_layer_model(X_train.shape[1], 1, lr=2)\n",
    "two_layer_model_1000.fit(X_1000_train, \n",
    "          y_1000_train, batch_size = 25,\n",
    "          epochs=2000, \n",
    "          verbose=0, \n",
    "          validation_data=(X_1000_test, y_1000_test), \n",
    "          callbacks=[plot_losses, checkpointer],\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_two_layer_model_1000 = plot_results(two_layer_model_1000, X_1000_train, y_1000_train, X_1000_test, y_1000_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Si lo comparamos contra el test set original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_two_layer_model_1000_test = two_layer_model_1000.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"loss: \", score_two_layer_model_1000_test[0])\n",
    "print(\"accuracy: \", score_two_layer_model_1000_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como soluciono el ruido? Learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses = PlotLosses(plot_interval=50, evaluate_interval=None, x_val=X_1000_test, y_val_categorical=y_1000_test)\n",
    "checkpointer = ModelCheckpoint(filepath='two_layer_model.1000.hdf5', verbose=0, save_best_only=True)\n",
    "two_layer_model_1000_lrd = get_two_layer_model(X_train.shape[1], 1, lr=2, decay=1e-3)\n",
    "two_layer_model_1000_lrd.fit(X_1000_train, \n",
    "          y_1000_train, batch_size = 25,\n",
    "          epochs=2000, \n",
    "          verbose=0, \n",
    "          validation_data=(X_1000_test, y_1000_test), \n",
    "          callbacks=[plot_losses, checkpointer],\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_two_layer_model_1000_lrd = plot_results(two_layer_model_1000_lrd, X_1000_train, y_1000_train, X_1000_test, y_1000_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_two_layer_model_1000__lrd_test = two_layer_model_1000_lrd.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"loss: \", score_two_layer_model_1000__lrd_test[0])\n",
    "print(\"accuracy: \", score_two_layer_model_1000__lrd_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Achico red neuronal (3 hidden units) - Volvemos al dataset de 200 muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses = PlotLosses(plot_interval=50, evaluate_interval=None, x_val=X_test, y_val_categorical=y_test)\n",
    "checkpointer = ModelCheckpoint(filepath='two_layer_model.h3.hdf5', verbose=0, save_best_only=True)\n",
    "two_layer_model_h3 = get_two_layer_model(X_train.shape[1], 1, hidden_units=3,lr=2, decay=1e-3)\n",
    "two_layer_model_h3.fit(X_train, \n",
    "          y_train, batch_size = 25,\n",
    "          epochs=3500, \n",
    "          verbose=0, \n",
    "          validation_data=(X_test, y_test), \n",
    "          callbacks=[plot_losses, checkpointer],\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_two_layer_model_1000_lrd = plot_results(two_layer_model_h3, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout (10 hidden units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_two_layer_model_dropout(input_shape, output_size, hidden_units=10, lr=0.1, prob=0.1, decay=0.0):\n",
    "    model = Sequential()\n",
    "    sgd = optimizers.SGD(lr=lr, decay=decay)\n",
    "    model.add(Dense(hidden_units,input_dim=input_shape,  activation='sigmoid', ))\n",
    "    model.add(Dropout(prob))\n",
    "    model.add(Dense(output_size, \n",
    "                    activation='sigmoid', \n",
    "                    kernel_initializer='zeros', \n",
    "                    name='Salida'\n",
    "                   ))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variar prob: 0.1, 0.2, 0.5\n",
    "plot_losses = PlotLosses(plot_interval=50, evaluate_interval=None, x_val=X_test, y_val_categorical=y_test)\n",
    "checkpointer = ModelCheckpoint(filepath='two_layer_model.dropout.hdf5', verbose=0, save_best_only=True)\n",
    "two_layer_model_dropout = get_two_layer_model_dropout(X_train.shape[1], 1, lr=2, decay=0.0, prob=0.5)\n",
    "two_layer_model_dropout.fit(X_train, \n",
    "          y_train, batch_size = 25,\n",
    "          epochs=5000, \n",
    "          verbose=0, \n",
    "          validation_data=(X_test, y_test), \n",
    "          callbacks=[plot_losses, checkpointer],\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_two_layer_model_1000_lrd = plot_results(two_layer_model_dropout, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variar prob: 0.1, 0.2, 0.5\n",
    "plot_losses = PlotLosses(plot_interval=50, evaluate_interval=None, x_val=X_test, y_val_categorical=y_test)\n",
    "checkpointer = ModelCheckpoint(filepath='two_layer_model.dropout.decay.hdf5', verbose=0, save_best_only=True)\n",
    "two_layer_model_dropout_decay = get_two_layer_model_dropout(X_train.shape[1], 1, lr=1, decay=0.0, prob=0.5)\n",
    "two_layer_model_dropout_decay.fit(X_train, \n",
    "          y_train, batch_size = 25,\n",
    "          epochs=5000, \n",
    "          verbose=0, \n",
    "          validation_data=(X_test, y_test), \n",
    "          callbacks=[plot_losses, checkpointer],\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_results(two_layer_model_dropout_decay, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_two_layer_model_L2(input_shape, output_size, hidden_units=10, lr=0.1, l2_lambda=0.1, decay=0.0):\n",
    "    model = Sequential()\n",
    "    sgd = optimizers.SGD(lr=lr, decay=decay)\n",
    "    regularizer = regularizers.l2(l2_lambda)\n",
    "    model.add(Dense(hidden_units,input_dim=input_shape,  activation='sigmoid', kernel_regularizer=regularizer, \n",
    "                    bias_regularizer=regularizer))\n",
    "    model.add(Dense(output_size, \n",
    "                    activation='sigmoid', \n",
    "                    kernel_initializer='zeros', \n",
    "                    name='Salida',\n",
    "                    kernel_regularizer=regularizer, \n",
    "                    bias_regularizer=regularizer\n",
    "                   ))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variar lambda: 0.1, 0.01, 0.005, 0.001\n",
    "plot_losses = PlotLosses(plot_interval=50, evaluate_interval=None, x_val=X_test, y_val_categorical=y_test)\n",
    "checkpointer = ModelCheckpoint(filepath='two_layer_model.l2.hdf5', verbose=0, save_best_only=True)\n",
    "two_layer_model_L2 = get_two_layer_model_L2(X_train.shape[1], 1, lr=2, decay=0.0, l2_lambda=0.0001)\n",
    "two_layer_model_L2.fit(X_train, \n",
    "          y_train, batch_size = 25,\n",
    "          epochs=5000, \n",
    "          verbose=0, \n",
    "          validation_data=(X_test, y_test), \n",
    "          callbacks=[plot_losses, checkpointer],\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_results(two_layer_model_L2, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_hist_weights(model):\n",
    "    weights = model.get_weights()\n",
    "    w1 = np.concatenate((weights[0].flatten(),weights[1].flatten()))\n",
    "    w2 = np.concatenate((weights[2].flatten(),weights[3].flatten()))\n",
    "    wf = np.concatenate((w1,w2))\n",
    "    plt.hist(wf,100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist_weights(two_layer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist_weights(two_layer_model_L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_two_layer_model_L1(input_shape, output_size, hidden_units=10, lr=0.1, l1_lambda=0.1, decay=0.0):\n",
    "    model = Sequential()\n",
    "    sgd = optimizers.SGD(lr=lr, decay=decay)\n",
    "    regularizer = regularizers.l1(l1_lambda)\n",
    "    model.add(Dense(hidden_units,input_dim=input_shape,  activation='sigmoid', \n",
    "                    kernel_regularizer=regularizer, \n",
    "                    bias_regularizer=regularizer))\n",
    "    model.add(Dense(output_size, \n",
    "                    activation='sigmoid', \n",
    "                    kernel_initializer='zeros', \n",
    "                    name='Salida',\n",
    "                    kernel_regularizer=regularizer, \n",
    "                    bias_regularizer=regularizer\n",
    "                   ))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variar lambda: 0.001, 0.005\n",
    "plot_losses = PlotLosses(plot_interval=50, evaluate_interval=None, x_val=X_test, y_val_categorical=y_test)\n",
    "checkpointer = ModelCheckpoint(filepath='two_layer_model.l1.hdf5', verbose=0, save_best_only=True)\n",
    "two_layer_model_L1 = get_two_layer_model_L1(X_train.shape[1], 1, lr=2, decay=0.0, l1_lambda=0.0005)\n",
    "two_layer_model_L1.fit(X_train, \n",
    "          y_train, batch_size = 25,\n",
    "          epochs=5000, \n",
    "          verbose=0, \n",
    "          validation_data=(X_test, y_test), \n",
    "          callbacks=[plot_losses, checkpointer],\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_results(two_layer_model_L1, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist_weights(two_layer_model_L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
