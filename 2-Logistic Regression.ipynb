{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/PAE-ITBA-ML-2019/Regularizacion/master/reg_helper.py\n",
    "! wget https://raw.githubusercontent.com/PAE-ITBA-ML-2019/Regularizacion/master/fnn_helper.py\n",
    "! wget https://raw.githubusercontent.com/PAE-ITBA-ML-2019/Regularizacion/master/draw_nn.py\n",
    "! wget https://raw.githubusercontent.com/PAE-ITBA-ML-2019/Regularizacion/master/regularization_helper.py\n",
    "\n",
    "! wget https://github.com/PAE-ITBA-ML-2019/Regularizacion/raw/master/data.zip\n",
    "! unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste de la cantidad de parámetros de un modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya sabemos que es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuando se da el overfitting?\n",
    "\n",
    "La cantidad de parámetros comparable con la cantidad de observaciones\n",
    "\n",
    "Estrategia natural: Reducir la cantidad de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.display import HTML\n",
    "import reg_helper as RHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import draw_nn\n",
    "from matplotlib import pyplot as plt \n",
    "folder = 'data/'\n",
    "X_train = np.load(folder+'X_train.npy')\n",
    "X_test = np.load(folder+'X_test.npy')\n",
    "y_train = np.load(folder+'y_train.npy') \n",
    "y_test = np.load(folder+'y_test.npy')\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,6))\n",
    "RHelper.plot_boundaries(X_train, y_train, ax=ax1)\n",
    "RHelper.plot_boundaries(X_test, y_test, ax=ax2)\n",
    "ax1.set_title('Train Data')\n",
    "ax2.set_title('CV Data')\n",
    "ax1.set_xlabel('$X_1$')\n",
    "ax1.set_ylabel('$X_2$')\n",
    "ax2.set_xlabel('$X_1$')\n",
    "ax2.set_ylabel('$X_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión logística - Underfitting - High Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = draw_nn.DrawNN( [2, 1] )\n",
    "network.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\large\n",
    "a = x_1w_1 + x_2w_2+w_0\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\large\n",
    "y = \\sigma(a)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 1\n",
    "_ = RHelper.fit_and_get_regions(X_train, y_train, X_test, y_test, degree=degree, lambd = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión logística polinomial - Overfitting - High Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = draw_nn.DrawNN( [10, 1] )\n",
    "network.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\large\n",
    "a = w_0 + x_1w_1 + x_2w_2 + x_1x_2w_3 + w_4x_1^2 + w_5x_2^2 + ... + w_Nx_1^K\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\large\n",
    "y = \\sigma(a)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "N = 18\n",
    "pol_orders = np.linspace(1, N, N)\n",
    "plt.plot(pol_orders, list(map(RHelper.params_vs_pol_order, pol_orders)))\n",
    "plt.xlabel('Orden del polinomio')\n",
    "plt.ylabel('Cantidad de parámetros')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar a 0.25\n",
    "lambd = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 18\n",
    "_ = RHelper.fit_and_get_regions(X_train, y_train, X_test, y_test, degree=degree, lambd = lambd, mesh_res=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aclaración: El overfitting se dá debido a la cantidad de parámetros, no debido a la cantidad de entradas (aumentar la cantidad de entradas hace que aumente la cantidad de parámetros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 6\n",
    "_ = RHelper.fit_and_get_regions(X_train, y_train, X_test, y_test, degree=degree, lambd = lambd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encontrar el orden de polinomio óptimo. Ni overfitting ni underfitting (Just Right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 2\n",
    "_ = RHelper.fit_and_get_regions(X_train, y_train, X_test, y_test, degree=degree, lambd = lambd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cual es el optimo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = [{'degree': 1, 'lambd': lambd}, \n",
    "           {'degree': 2, 'lambd': lambd}, \n",
    "           {'degree': 3, 'lambd': lambd}, \n",
    "           {'degree': 4, 'lambd': lambd}, \n",
    "           {'degree': 5, 'lambd': lambd}, \n",
    "           {'degree': 6, 'lambd': lambd}, \n",
    "           {'degree': 7, 'lambd': lambd}, \n",
    "           {'degree': 8, 'lambd': lambd}, \n",
    "           {'degree': 9, 'lambd': lambd}, \n",
    "           {'degree': 10, 'lambd': lambd},\n",
    "           {'degree': 11, 'lambd': lambd},\n",
    "           {'degree': 12, 'lambd': lambd},\n",
    "           {'degree': 13, 'lambd':lambd},\n",
    "           {'degree': 14, 'lambd': lambd}, \n",
    "           {'degree': 15, 'lambd': lambd}, \n",
    "           {'degree': 16, 'lambd': lambd}, \n",
    "           {'degree': 17, 'lambd': lambd}, \n",
    "           {'degree': 18, 'lambd': lambd}\n",
    "           ]\n",
    "degrees_0, lambdas_0, train_acc_array_0, test_acc_array_0, coefs_array_mean_0, coefs_array_std_0, coefs_abs_max_0, coefs_norm, coefs_num = RHelper.test_options(X_train, y_train, X_test, y_test, options, plot_it=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,5))\n",
    "ax.plot(degrees_0, coefs_num)\n",
    "ax.set_title(\"Cantidad de parametros en función del orden del polinomio\")\n",
    "ax.set_xticks(degrees_0)\n",
    "plt.show()\n",
    "fig, ax = plt.subplots(figsize=(20,5))\n",
    "ax.plot(degrees_0, train_acc_array_0, label=\"Train\")\n",
    "ax.plot(degrees_0, test_acc_array_0, label=\"Test\")\n",
    "ax.set_title(\"Accuracy en función del orden del polinomio\")\n",
    "ax.set_xlabel('Orden del polinomio')\n",
    "ax.set_xticks(degrees_0)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(degrees_0, coefs_array_mean_0)\n",
    "plt.title(\"Media de los pesos en función del orden del polinomio\")\n",
    "plt.xticks(degrees_0)\n",
    "plt.show()\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(degrees_0, coefs_array_std_0)\n",
    "plt.title(\"STDs de los pesos\")\n",
    "plt.xticks(degrees_0)\n",
    "plt.show()\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(degrees_0, coefs_abs_max_0)\n",
    "plt.title(\"Maximo de los pesos\")\n",
    "plt.xticks(degrees_0)\n",
    "plt.show()\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(degrees_0, coefs_norm)\n",
    "plt.title(\"Norma de vector de pesos\")\n",
    "plt.xticks(degrees_0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preguntas:\n",
    "- Cual es la cantidad de parámetros óptima?\n",
    "- Que pasa con los pesos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Que pasa si reducimos el valor de los parámetros aprendidos (pesos)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 1e-10\n",
    "clf_logist_pol = LogisticRegression(C=1/lambd, fit_intercept=False, solver='lbfgs', max_iter=2000)\n",
    "\n",
    "degree = 18\n",
    "X_train_degree = RHelper.get_polynimial_set(X_train, degree=degree)\n",
    "X_test_degree = RHelper.get_polynimial_set(X_test, degree=degree)\n",
    "clf_logist_pol.fit(X_train_degree, y_train)\n",
    "\n",
    "score_train_logist_pol = clf_logist_pol.score(X_train_degree, y_train)\n",
    "score_test_logist_pol = clf_logist_pol.score(X_test_degree, y_test)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,6))\n",
    "RHelper.plot_boundaries(X_train, y_train, score_train_logist_pol, clf_logist_pol.predict_proba, degree=degree, ax=ax1, mesh_res = 200)\n",
    "RHelper.plot_boundaries(X_test, y_test, score_test_logist_pol, clf_logist_pol.predict_proba, degree=degree, ax=ax2, mesh_res = 200)\n",
    "print('Regresion Logistica Polinomial de orden '+str(degree) +', con lamdba (regularización L2):' +  str(lambd))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = log_loss(y_train, clf_logist_pol.predict_proba(X_train_degree))\n",
    "test_loss = log_loss(y_test, clf_logist_pol.predict_proba(X_test_degree))\n",
    "print(f'Train loss: {train_loss}, Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_logist_pol.coef_ = clf_logist_pol.coef_/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,6))\n",
    "RHelper.plot_boundaries(X_train, y_train, score_train_logist_pol, clf_logist_pol.predict_proba, degree=degree, ax=ax1, mesh_res = 200)\n",
    "RHelper.plot_boundaries(X_test, y_test, score_test_logist_pol, clf_logist_pol.predict_proba, degree=degree, ax=ax2, mesh_res = 200)\n",
    "print('Regresion Logistica Polinomial de orden '+str(degree) +', con lamdba (regularización L2):' +  str(lambd))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que pasa con el accuracy y la loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = log_loss(y_train, clf_logist_pol.predict_proba(X_train_degree))\n",
    "test_loss = log_loss(y_test, clf_logist_pol.predict_proba(X_test_degree))\n",
    "print(f'Train loss: {train_loss}, Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video recomendado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo video Udacity:\n",
    "https://youtu.be/aX_m9iyK3Ac\n",
    "\n",
    "<img src=\"images/2points.png\" alt=\"Drawing\" style=\"width:60%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sigmoideas.png\" alt=\"Drawing\" style=\"width:60%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuición: Si mantengo el valor de los pesos bajos -> eventualmente trabajo en la parte lineal, modelo lineal -> Disminuye overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión de Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\large J_R(w) = -\\frac{1}{N}\\sum_{i}^N y_nlog[\\sigma(x_i^Tw)] + (1-y_n)log[1-\\sigma(x_i^Tw)] + \\lambda \\sum_{i}^d{w_i}^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\large J_R(w) = J(w) + \\lambda \\sum_{i}^d{w_i}^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "volver y resolver problema cambiando lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
